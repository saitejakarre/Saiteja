{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsW3gHz4LP1gKBpIcmJR7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saitejakarre/Saiteja/blob/main/NER_end_to_end_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "zDhrfYD4jQC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "I4FWU9vbjW4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"datasets<4.0.0\""
      ],
      "metadata": {
        "id": "LVVZn5XbkEo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"conll2003\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "s1grRhhJjge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "gzwgtRS0j3U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "num_labels = len(label_list)"
      ],
      "metadata": {
        "id": "pzEf4gHcka8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "dyC1lK01kdSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")"
      ],
      "metadata": {
        "id": "jK6-G0zzkgQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to assign the splitted tokens to their original words\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_id = None\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "\n",
        "                label_ids.append(-100)\n",
        "            elif word_id != previous_word_id:\n",
        "                label_ids.append(label[word_id])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_id = word_id\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "cSgXDCeFklW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "RIjY9LyalCZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "OO-ANEjWlEo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"evaluate==0.4.0\"\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "ZrAhzRv3lNwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "twx_eRcdlgZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "feOOrqGDlP8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "voEdrAWXlrN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner-distilbert\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    logging_steps=10\n",
        ")"
      ],
      "metadata": {
        "id": "FjIwwcGUlt_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "gVh8-RH0l0WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "JxcEWLAil6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./ner-distilbert\")"
      ],
      "metadata": {
        "id": "CIgtsOocm0np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "jVmwFmn5oVPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"./ner-distilbert\",\n",
        "    tokenizer=\"./ner-distilbert\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ArlSQF4YoaW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_labels(results):\n",
        "    decoded = []\n",
        "    for ent in results:\n",
        "        label_id = int(ent[\"entity_group\"].split(\"_\")[-1])\n",
        "        ent[\"entity_group\"] = label_list[label_id]\n",
        "        decoded.append(ent)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "n0mrj6s8o0-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"SaiTeja living in Tirumala with lord venkateshwara.\"\n",
        "raw_results = ner_pipeline(sentence)"
      ],
      "metadata": {
        "id": "mMq64_ZaodsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_results = decode_labels(raw_results)"
      ],
      "metadata": {
        "id": "xMsbLtnRogCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in final_results:\n",
        "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}\")\n"
      ],
      "metadata": {
        "id": "kZJpZJQXoh5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}